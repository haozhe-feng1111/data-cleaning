# 中文维基百科数据清洗项目

本项目用于下载、解析和清洗中文维基百科数据，将其转换为适合预训练模型使用的JSONL格式。

## 项目概述

- **数据源**: 中文维基百科 20250201 dump
- **数据地址**: https://dumps.wikimedia.org/zhwiki/20250201/
- **输出格式**: JSONL，结构为 `{"text": "", "meta": {}}`
- **目标**: 生成高质量的中文文本数据用于语言模型预训练

## 项目结构

```
kimi笔试/
├── main.py              # 主处理脚本
├── downloader.py        # 维基百科数据下载器
├── parser.py            # XML解析器
├── cleaner.py           # 文本清洗工具
├── formatter.py         # JSONL格式化输出
├── run_example.py       # 示例运行脚本
├── requirements.txt     # 依赖包列表
├── README.md           # 项目说明文档
├── data/               # 下载的原始数据目录
└── output/             # 输出文件目录
    ├── cleaned_wikipedia.jsonl  # 完整清洗后数据
    └── sample_1000.jsonl       # 1000条样例数据
```

## 安装依赖

```bash
pip install -r requirements.txt
```

## 使用方法

### 1. 快速开始（生成示例数据）

```bash
python run_example.py
```

这将生成包含1000条示例记录的样本文件，用于演示数据格式和清洗效果。

### 2. 完整数据处理流程

#### 下载并处理完整数据集：
```bash
python main.py --download --max-articles 10000 --sample 1000
```

#### 仅下载数据：
```bash
python main.py --download
```

#### 处理已下载的数据文件：
```bash
python main.py --process data/zhwiki-20250201-pages-articles.xml.bz2 --max-articles 5000
```

#### 创建样本文件：
```bash
python main.py --process data/zhwiki-20250201-pages-articles.xml.bz2 --sample 1000
```

### 3. 参数说明

- `--download`: 下载维基百科dump文件
- `--process DUMP_FILE`: 处理指定的dump文件
- `--max-articles N`: 限制处理的文章数量
- `--sample N`: 创建包含N条记录的样本文件（默认1000）
- `--output-dir DIR`: 指定输出目录（默认output）
- `--data-dir DIR`: 指定数据下载目录（默认data）

## 数据清洗策略

### 1. 文章过滤策略

- **跳过重定向页面**: 过滤 `#重定向` 和 `#REDIRECT` 页面
- **跳过消歧义页面**: 过滤标题包含"消歧义"的页面
- **命名空间过滤**: 跳过非正文命名空间（如User、Talk、Template等）
- **最小长度要求**: 跳过原始文本少于200字符的短文章

### 2. 文本清洗规则

#### Wikipedia标记清理：
- 移除HTML标签：`<ref>`, `<comment>` 等
- 移除模板和信息框：`{{Infobox}}`, `{{Template}}` 等
- 移除分类标签：`[[Category:...]]`
- 移除文件链接：`[[File:...]]`, `[[Image:...]]`
- 移除表格标记：`{| ... |}`

#### 文本质量提升：
- 移除URL和邮箱地址
- 清理引用标记：`[1]`, `[2]` 等
- 处理维基链接：`[[链接|显示文本]]` → `显示文本`
- 移除导航元素和模板残留
- 规范化空白字符

#### 段落级别筛选：
- 最小段落长度：20字符
- 中文字符比例检查：至少30%中文字符
- 过滤主要由标点符号组成的段落
- 移除重复度过高的内容

### 3. 质量检查

#### 最终质量要求：
- 清洗后文本至少100字符
- 至少包含20个中文字符
- 至少3个非空行
- 词汇重复度检查（防止垃圾内容）

## 输出格式

每条记录为JSONL格式，包含以下字段：

```json
{
    "text": "清洗后的文章正文内容",
    "meta": {
        "title": "文章标题",
        "page_id": "维基百科页面ID",
        "char_count": 1250,
        "source": "zh_wikipedia_20250201",
        "categories": ["分类1", "分类2"],
        "timestamp": "2025-02-01T00:00:00Z"
    }
}
```

## 技术实现

### 核心组件

1. **WikiDumpDownloader**: 负责从维基媒体服务器下载压缩的XML dump文件
2. **WikiXMLParser**: 使用流式解析处理大型XML文件，内存效率高
3. **TextCleaner**: 实现多层文本清洗算法，使用编译后的正则表达式提高性能
4. **JSONLFormatter**: 处理输出格式化、验证和统计

### 性能优化

- 流式XML解析，支持处理GB级别的dump文件
- 编译正则表达式模式，提高清洗效率
- 增量式文件写入，避免内存溢出
- 进度条显示，实时监控处理状态

### 错误处理

- XML解析异常处理
- 网络下载重试机制
- 文件格式验证
- 清洗质量检查

## 处理统计

运行过程中会显示详细的处理统计信息：

- 处理文章总数
- 成功写入记录数
- 跳过记录数及原因
- 处理速度（记录/秒）
- 输出文件大小
- 数据质量验证结果

## 常见问题解决

### 1. 下载失败
- 检查网络连接
- 验证dump文件URL是否正确
- 确保有足够的磁盘空间

### 2. 内存不足
- 使用 `--max-articles` 参数限制处理量
- 流式处理设计应该能处理大文件，但极端情况下可分批处理

### 3. 清洗质量问题
- 调整 `cleaner.py` 中的质量检查参数
- 检查正则表达式模式是否适合数据特点
- 验证中文字符比例要求是否合理

## 样例数据

项目包含1000条清洗后的样例数据，展示了：
- 各类维基百科文章的清洗效果
- 不同长度和复杂度的内容处理
- 元数据提取的完整性

## 扩展功能

可以根据需要扩展的功能：
- 支持其他语言的维基百科
- 添加更多元数据字段提取
- 实现并行处理提高速度
- 集成其他文本质量评估指标

## 许可证

本项目用于学术和研究目的。维基百科数据遵循CC BY-SA 3.0许可证。